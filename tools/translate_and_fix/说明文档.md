## 各文件功能概览
- LLM_config.yaml: 存放大模型配置信息
- LLM_calling.py: 调用大模型及相关配置准备
- function_translation.py: 函数翻译
- function_syntax_fixing.py: 函数语法修复
- function_semantic_fixing.py: 函数语义修复（不一定会用到，语义修复主要交给人工来完成）
- parse_LLM_output.py: 解析大模型输出
- translate_and_fix.py: 工具调用脚本

## 各接口输入输出内容&格式
通过命令行调用各翻译/修复工具。

各参数说明：
```
--config: 配置文件的路径。根据会上讨论结果，调用翻译工具时，工作路径应该在.c2rust目录的父目录，请根据该原则计算配置文件路径。如果以上原则需要调整，
          则只需要相应地调整每次调用翻译工具时传入这里的路径字符串，无需做其他调整。
--type: 要调用的工具类型。只允许var/fn/fix三种输入。var即为调用变量翻译，fn即为调用函数翻译，fix即为调用语法修复。
--code: 要输入给大模型的代码所在的文件的路径。
        对于变量/函数翻译，该文件内应该有且仅有待翻译的C代码；
        对于语法修复，该文件内应该有且仅有待修复的Rust代码。（同一次LLM翻译输出的相关导入代码（例如use crate::*)或FFI声明代码也可以在里面）
--output: 大模型翻译/修复结果应输出到的文件的路径。用于存储本次大模型翻译/修复的结果，供集成工具做后续处理。
          翻译工具会对大模型输出做简单的处理，保证只有Rust代码及相关注释会被写入该文件。
          翻译/修复结果会覆盖该文件内的原有内容！
--error: 调用语法修复工具时，编译器报错信息所在的文件的路径。该文件应包括本次编译的所有且完整的报错信息。
         仅在调用语法修复工具时，需要填写该参数。
```
### 调用变量翻译工具
调用示例：
`python translate_and_fix.py --config config.toml --type var --code code.c --output output.rs`

### 调用函数翻译工具
调用示例：
`python translate_and_fix.py --config config.toml --type fn --code code.c --output output.rs`

### 调用语法修复工具
调用示例：
`python translate_and_fix.py --config config.toml --type fix --code code.rs --output output.rs --error error.txt`

## 各文件功能说明
### LLM_config.yaml
在这里存放大模型的配置信息，这些信息用于工具初始化时创建与大模型平台的api调用会话，从而在翻译过程中调用大模型。现在主要包括以下配置项：

    - `model`: 大模型名称，告诉API使用哪一个具体的模型来处理请求。
    - `base_url`: 大模型平台的URL，让程序知道在哪里发送请求，访问大模型的服务。
    - `api_key`: 调用大模型时用于身份验证的秘钥，通过平台的访问认证，并且确保会话的一致性，防止被篡改或被他人冒用。
    - `temperature`: 温度，用于控制模型生成文本的随机性。温度越高，模型生成的文本越多样化，但可能会不准确；温度越低，模型生成的文本越确定，但可能缺乏创意。
    - `top_p`: 用于控制生成文本的多样性，表示在生成下一个词时，选择概率最高的前 p% 的词进行采样。设置为 1，意味着考虑所有可能的词，生成的文本会更加保守。
    - `seed`: 随机数种子，固定一个随机数种子有利于确保结果的可重复性（但仍不能保证两次实验的结果完全一致）
    - `max_retries`: 在API调用失败时，最多重试的次数。在网络请求中，可能会因为网络波动、服务器繁忙等原因导致请求失败。配置该参数可以让程序在遇到失败时自动重试，直到成功或达到最大重试次数，从而提高API调用的可靠性。
    - `timeout`: API调用的超时时间，如果在设置的时间内没有收到响应，程序就会认为请求超时，停止等待，从而避免长时间挂起，影响程序的性能和用户体验。

### LLM_calling.py
#### class LLM_config
用于存放从`LLM_config.yaml`读取的配置信息，各成员变量即为`LLM_config.yaml`中的各配置项。

#### load_LLM_config(config_file_path)
根据入参`config_file_path`（即`LLM_config.yaml`文件的路径）读取大模型配置信息，并将它们存放到一个`LLM_config`对象中。

#### init_openai_instance(LLM_config)
创建一个与大模型平台的api调用会话。入参`LLM_config`提供了该会话的配置信息。

#### call_llm_api(prompt, model, openai_instance)
通过api访问大模型平台进行对话，获取并返回大模型平台的回复。`prompt`即为我们这次对话给大模型的输入，`model`为访问的大模型名称，`openai_instance`为`init_openai_instance()`创建的会话对象。

### function_translation.py
#### function_translation(c_function_code, model, openai_instance)
构建函数翻译prompt，并调用大模型将C函数翻译为Rust函数，最后从大模型输出中提取出Rust函数以及该函数调用的函数名列表。

### function_syntax_fixing.py
#### function_syntax_fixing(rust_function_code, error_message, model, openai_instance)
构建函数语法修复的prompt，并调用大模型根据Rust代码与语法错误信息修复Rust函数，最后从大模型输出中提取出修复后的Rust函数。

### function_semantic_fixing.py
#### function_semantic_fixing(c_function_code, rust_function_code, c_test_case_code, model, openai_instance)
构建函数语义修复的prompt，并调用大模型根据Rust代码、原始C代码、以及未通过的测试用例代码修复Rust函数，最后从大模型输出中提取出修复后的Rust函数。

### parse_LLM_output.py
#### match_rust_function(output)
从大模型的回复`output`中提取出Rust函数并返回。

#### match_dependencies(output)
从大模型的回复`output`中提取出依赖项列表并返回。