## 各文件功能概览
- LLM_config.yaml: 存放大模型配置信息
- LLM_calling.py: 调用大模型及相关配置准备
- function_translation.py: 函数翻译
- function_syntax_fixing.py: 函数语法修复
- function_semantic_fixing.py: 函数语义修复（不一定会用到，语义修复主要交给人工来完成）
- parse_LLM_output.py: 解析大模型输出

## 各接口输入输出内容&格式
以下各接口的输入输出内容&格式在各接口的`.py`文件中有示例用法可供参考。
### 1.function_translation(c_function_code, model, openai_instance)
- **c_function_code**： `str`；带翻译C函数，应仅输入待翻译的C函数的完整源代码，不再包括其他任何内容
  - e.g.:
```python
c_function_code = """
    void my_function() {
        printf("Hello from C!");
    }
    """
```
- **model**: `str`；从`LLM_config.yaml`读取得到的大模型名称
- **openai_instance**: `class openai.OpenAI`；通过`LLM_calling.py`中`init_openai_instance()`函数创建的api调用实例，用于调用大模型
- **返回值rust_function, dependencies**: `str, str`；第一个是翻译出的Rust函数，第二个是该函数调用的函数名列表

### 2.function_syntax_fixing(rust_function_code, error_message, model, openai_instance)
- **rust_function_code**: `str`；需要语法修复的Rust函数，应仅输入待修复的Rust函数的完整代码，不再包括其他任何内容
    - e.g.:
```python
rust_function_code = """
    fn my_function() {
        println!("Hello from Rust!")
    }
    """
```
- **error_message**: `str`；编译报错信息
  - e.g.:
```python
error_message = """
    error: expected `;` in function
    ---> src/main.rs
    1 | fn my_function() {
    2 |     println!("Hello from Rust!")
      |     ---------------------------^
      |     |                           |
      |     |                           expected `;` here
      |     consider add a semicolon here
    """
```
- **model**： `str`；从`LLM_config.yaml`读取得到的大模型名称
- **openai_instance**： `class openai.OpenAI`；通过`LLM_config.py`中`init_openai_instance()`函数创建的api调用实例，用于调用大模型
- **返回值rust_function**: `str`；修复后的Rust函数

### 3.function_semantic_fixing(C_function_code, rust_function_code, c_test_case_code, model, openai_instance)
- **c_function_code**: `str`；翻译前的C函数，用于为大模型语义修复时提供一个语义正确的参考。应仅输入翻译前的C函数的完整源代码，不再包括其他任何内容
    - e.g.:
```python
c_function_code = """
    void my_function() {
        printf("Hello from C!");
    }
    """
```
- **rust_function_code**: `str`；需要语义修复的Rust函数，应仅输入待修复的Rust函数的完整代码，不再包括其他任何内容
  - e.g.:
```python
rust_function_code = """
    fn my_function() {
        println!("Hello from Rust!")
    }
    """
```
- **c_test_case_code**: `str`；未通过的C测试用例的完整代码
- **model**: `str`；从`LLM_config.yaml`读取得到的大模型名称
- **openai_instance**： `class openai.OpenAI`；通过`LLM_config.py`中`init_openai_instance()`函数创建的api调用实例，用于调用大模型
- **返回值rust_function**: `str`；修复后的Rust函数

## 各文件功能说明
### LLM_config.yaml
在这里存放大模型的配置信息，这些信息用于工具初始化时创建与大模型平台的api调用会话，从而在翻译过程中调用大模型。现在主要包括以下配置项：

    - `model`: 大模型名称，告诉API使用哪一个具体的模型来处理请求。
    - `base_url`: 大模型平台的URL，让程序知道在哪里发送请求，访问大模型的服务。
    - `api_key`: 调用大模型时用于身份验证的秘钥，通过平台的访问认证，并且确保会话的一致性，防止被篡改或被他人冒用。
    - `temperature`: 温度，用于控制模型生成文本的随机性。温度越高，模型生成的文本越多样化，但可能会不准确；温度越低，模型生成的文本越确定，但可能缺乏创意。
    - `top_p`: 用于控制生成文本的多样性，表示在生成下一个词时，选择概率最高的前 p% 的词进行采样。设置为 1，意味着考虑所有可能的词，生成的文本会更加保守。
    - `seed`: 随机数种子，固定一个随机数种子有利于确保结果的可重复性（但仍不能保证两次实验的结果完全一致）
    - `max_retries`: 在API调用失败时，最多重试的次数。在网络请求中，可能会因为网络波动、服务器繁忙等原因导致请求失败。配置该参数可以让程序在遇到失败时自动重试，直到成功或达到最大重试次数，从而提高API调用的可靠性。
    - `timeout`: API调用的超时时间，如果在设置的时间内没有收到响应，程序就会认为请求超时，停止等待，从而避免长时间挂起，影响程序的性能和用户体验。

### LLM_calling.py
#### class LLM_config
用于存放从`LLM_config.yaml`读取的配置信息，各成员变量即为`LLM_config.yaml`中的各配置项。

#### load_LLM_config(config_file_path)
根据入参`config_file_path`（即`LLM_config.yaml`文件的路径）读取大模型配置信息，并将它们存放到一个`LLM_config`对象中。

#### init_openai_instance(LLM_config)
创建一个与大模型平台的api调用会话。入参`LLM_config`提供了该会话的配置信息。

#### call_llm_api(prompt, model, openai_instance)
通过api访问大模型平台进行对话，获取并返回大模型平台的回复。`prompt`即为我们这次对话给大模型的输入，`model`为访问的大模型名称，`openai_instance`为`init_openai_instance()`创建的会话对象。

### function_translation.py
#### function_translation(c_function_code, model, openai_instance)
构建函数翻译prompt，并调用大模型将C函数翻译为Rust函数，最后从大模型输出中提取出Rust函数以及该函数调用的函数名列表。

### function_syntax_fixing.py
#### function_syntax_fixing(rust_function_code, error_message, model, openai_instance)
构建函数语法修复的prompt，并调用大模型根据Rust代码与语法错误信息修复Rust函数，最后从大模型输出中提取出修复后的Rust函数。

### function_semantic_fixing.py
#### function_semantic_fixing(c_function_code, rust_function_code, c_test_case_code, model, openai_instance)
构建函数语义修复的prompt，并调用大模型根据Rust代码、原始C代码、以及未通过的测试用例代码修复Rust函数，最后从大模型输出中提取出修复后的Rust函数。

### parse_LLM_output.py
#### match_rust_function(output)
从大模型的回复`output`中提取出Rust函数并返回。

#### match_dependencies(output)
从大模型的回复`output`中提取出依赖项列表并返回。